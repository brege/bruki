{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Screenshot Categorization\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "**Label**: manually assigned categories from the `www/` labeling tool. Semantic and contextual, not perceptual. These are the training targets for all classifiers.\n",
    "\n",
    "**Feature**: a signal dimension used as input to a model. TF-IDF weights are features. CLIP embeddings are features. Raw pixels are not.\n",
    "\n",
    "**Embedding**: the 768-dim vector CLIP produces per image. Encodes visual structure as learned from web-scale image-text pairs. No label semantics attached.\n",
    "\n",
    "**Prediction**: the output of a classifier against a defined label set.\n",
    "\n",
    "**Probability / score**: the confidence value attached to a prediction.\n",
    "\n",
    "## Models\n",
    "\n",
    "Two independent signals are extracted per screenshot and used to train separate classifiers against the same manual labels:\n",
    "\n",
    "**Text**: OCR transcript vectorized as TF-IDF char n-grams. Captures lexical density, repeated UI tokens, numeric patterns, and domain-specific vocabulary.\n",
    "\n",
    "**Image**: 768-dim embedding from `openai/clip-vit-base-patch32`. CLIP was pretrained contrastively on web-scraped image-text pairs, making it substantially more appropriate for screenshots than ImageNet-pretrained models. Captures visual geometry and layout.\n",
    "\n",
    "Both signals have different coverage over the label set. The eventual fusion model combines surviving signals per label based on measured separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def stopwatch(flag):\n",
    "    if flag:\n",
    "        stopwatch.s = time.perf_counter()\n",
    "    else:\n",
    "        print(time.perf_counter() - stopwatch.s)\n",
    "\n",
    "\n",
    "stopwatch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Set pathing and shared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(marker=\"data/labels.jsonl\"):\n",
    "    for p in (Path.cwd(), *Path.cwd().parents):\n",
    "        if (p / marker).exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(marker)\n",
    "\n",
    "\n",
    "ROOT = find_root()\n",
    "LABELS_PATH = ROOT / \"data/labels.jsonl\"\n",
    "\n",
    "rows = [json.loads(line) for line in LABELS_PATH.read_text().splitlines() if line.strip()]\n",
    "paths = [ROOT / r[\"input_path\"] for r in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "- `make_df` wraps classifier output into a labeled DataFrame indexed by filename.\n",
    "- `top_k` slices that DataFrame by label column\n",
    "- and returns ranked hits as a list\n",
    "of `{path, score}` dicts  the currency everything else operates on.\n",
    "- `top_labels` picks the *n* highest-mass columns by summing probability across\n",
    "all images. This is useful for a first-pass survey of what the model is confident about.\n",
    "- `print_hits` and `plot_hits` produce table and gallery of images and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "def make_df(probs, binarizer):\n",
    "    return pd.DataFrame(probs, columns=binarizer.classes_, index=[p.name for p in paths])\n",
    "\n",
    "\n",
    "def top_k(df, label, k=12):\n",
    "    scores = df[label].to_numpy()\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [{\"path\": paths[i], \"score\": scores[i]} for i in order]\n",
    "\n",
    "\n",
    "def top_labels(df, n=5):\n",
    "    return df.sum().nlargest(n).index.tolist()\n",
    "\n",
    "\n",
    "def print_hits(hits):\n",
    "    for h in hits:\n",
    "        print(f\"{h['score']:.4f}  {h['path'].parent.name}/{h['path'].name}\")\n",
    "\n",
    "\n",
    "def plot_hits(hits, columns=4, label=\"\"):\n",
    "    rows_ = (len(hits) + columns - 1) // columns\n",
    "    fig, axes = plt.subplots(rows_, columns, figsize=(3 * columns, 3 * rows_))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, hit in zip(axes, hits):\n",
    "        ax.imshow(Image.open(hit[\"path\"]))\n",
    "        ax.set_title(f\"{hit['score']:.3f}\\n{hit['path'].name}\")\n",
    "        ax.axis(\"off\")\n",
    "    for ax in axes[len(hits) :]:\n",
    "        ax.axis(\"off\")\n",
    "    if label:\n",
    "        fig.suptitle(label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def split_indices(n):\n",
    "    idx = np.random.RandomState(SEED).permutation(n)\n",
    "    cut = int(n * (1 - TEST_SIZE))\n",
    "    return idx[:cut], idx[cut:]\n",
    "\n",
    "\n",
    "def ensure_label_coverage(targets, train_idx, test_idx):\n",
    "    train_set, test_set = set(train_idx.tolist()), set(test_idx.tolist())\n",
    "    for col in range(targets.shape[1]):\n",
    "        if targets[list(train_set), col].sum() > 0:\n",
    "            continue\n",
    "        candidates = [i for i in test_set if targets[i, col] == 1]\n",
    "        if candidates:\n",
    "            test_set.remove(candidates[0])\n",
    "            train_set.add(candidates[0])\n",
    "    return np.array(sorted(train_set)), np.array(sorted(test_set))\n",
    "\n",
    "\n",
    "def numeric_density(text):\n",
    "    return sum(c.isdigit() for c in text) / max(len(text), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = Counter(label for r in rows for label in r[\"categories\"])\n",
    "s = pd.Series(label_counts).sort_values()\n",
    "print(f\"total labels: {len(s)}\")\n",
    "print(f\"labels with <5  examples: {(s < 5).sum()}\")\n",
    "print(f\"labels with <10 examples: {(s < 10).sum()}\")\n",
    "print(f\"labels with >=5 examples: {(s >= 5).sum()}\")\n",
    "print(f\"labels with >=10 examples: {(s >= 10).sum()}\")\n",
    "\n",
    "viable = s[s >= 5].index.tolist()\n",
    "print(f\"\\nviable labels: {viable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Text model\n",
    "\n",
    "OCR transcripts are cheap to extract and carry three distinct signal types depending on the label: \n",
    "\n",
    "1. lexical content (specific words and phrases)\n",
    "2. numeric density (scores, metrics, timestamps), and\n",
    "3. repeated UI tokens (OCR artifacts from icons, buttons, status indicators that are consistent within an app).\n",
    "\n",
    "The corpus is first profiled per label to establish whether any of these signal types are present before choosing a vectorizer. A sweep over vectorizer configurations and classifiers on the viable label subset determines the training configuration empirically.\n",
    "\n",
    "`char(2,4)` n-grams with logistic regression is the sweep winner on macro F1 across the expanded viable set (with >=5 examples). Macro is the target metric because it weights all labels equally regardless of frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Run OCR\n",
    "\n",
    "this part will take some time, but only once since this method has caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(ROOT / \"ml\"))\n",
    "import pytesseract\n",
    "from handler import load_config, resolve_image_paths\n",
    "\n",
    "config = load_config(ROOT / \"config.yaml\")\n",
    "resolved = resolve_image_paths(config, series_prefix=\"screenshot\")\n",
    "all_image_paths = [r.path for r in resolved]\n",
    "\n",
    "print(f\"resolved {len(all_image_paths)} images\")\n",
    "\n",
    "\n",
    "def ocr_image(path: Path, psm: int = 6, oem: int = 3) -> str:\n",
    "    with Image.open(path) as img:\n",
    "        if img.size[0] < 10 or img.size[1] < 10:\n",
    "            return \"\"\n",
    "        return pytesseract.image_to_string(\n",
    "            img.convert(\"RGB\"), lang=\"eng\", config=f\"--psm {psm} --oem {oem}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def ocr_output_path(image_path: Path) -> Path:\n",
    "    out = ROOT / \"data/ocr\" / image_path.parent.name / (image_path.stem + \".txt\")\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_ocr_incremental(image_paths: list[Path]) -> dict[Path, Path]:\n",
    "    mapping = {}\n",
    "    for path in tqdm(image_paths, desc=\"ocr\"):\n",
    "        out = ocr_output_path(path)\n",
    "        if not out.exists():\n",
    "            try:\n",
    "                text = ocr_image(path)\n",
    "            except (OSError, Exception) as e:\n",
    "                print(f\"\\nskipped {path.name}: {e}\")\n",
    "                text = \"\"\n",
    "            out.write_text(text, encoding=\"utf-8\")\n",
    "        mapping[path] = out\n",
    "    return mapping\n",
    "\n",
    "\n",
    "ocr_map = run_ocr_incremental(all_image_paths)\n",
    "print(f\"ocr complete: {len(ocr_map)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Load OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_path(r):\n",
    "    img = Path(r[\"input_path\"])\n",
    "    return ROOT / \"data/ocr\" / img.parent.name / (img.stem + \".txt\")\n",
    "\n",
    "\n",
    "text_ocr = [ocr_path(r).read_text().lower() for r in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Corpus: per-label OCR character profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in viable:\n",
    "    indices = [i for i, r in enumerate(rows) if label in r[\"categories\"]]\n",
    "    subset = [text_ocr[i] for i in indices]\n",
    "    lengths = [len(t.split()) for t in subset]\n",
    "    densities = [numeric_density(t) for t in subset]\n",
    "    empty = sum(1 for t in subset if len(t.strip()) < 20)\n",
    "    print(\n",
    "        f\"{label:20} n={len(indices):3}  mean_words={np.mean(lengths):6.0f}  \"\n",
    "        f\"mean_density={np.mean(densities):.3f}  empty={empty}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Top word tokens per viable label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=\"word\", min_df=1)\n",
    "X_cv = cv.fit_transform(text_ocr)\n",
    "vocab = np.array(cv.get_feature_names_out())\n",
    "\n",
    "for label in viable:\n",
    "    indices = [i for i, r in enumerate(rows) if label in r[\"categories\"]]\n",
    "    sums = np.asarray(X_cv[indices].sum(axis=0)).squeeze()\n",
    "    top = np.argsort(sums)[::-1][:15]\n",
    "    print(f\"\\n--- {label} ---\")\n",
    "    print(\", \".join(f\"{vocab[i]}({sums[i]:.0f})\" for i in top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Label Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), max_df=0.95)\n",
    "word_vec = TfidfVectorizer(analyzer=\"word\", min_df=1, max_df=0.95)\n",
    "char_feat = char_vec.fit_transform(text_ocr)\n",
    "word_feat = word_vec.fit_transform(text_ocr)\n",
    "\n",
    "stats_idx = [i for i, r in enumerate(rows) if \"statistics\" in r[\"categories\"]]\n",
    "monitor_idx = [\n",
    "    i\n",
    "    for i, r in enumerate(rows)\n",
    "    if \"monitorat\" in r[\"categories\"] and \"statistics\" not in r[\"categories\"]\n",
    "]\n",
    "\n",
    "for name, feat in [(\"char\", char_feat), (\"word\", word_feat)]:\n",
    "    sim = cosine_similarity(feat[stats_idx], feat[monitor_idx])\n",
    "    print(f\"{name}  statistics vs monitorat: mean={sim.mean():.3f} max={sim.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### vectorizer + classifier sweep\n",
    "\n",
    "the purpose here is to determine the best model for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "viable_set = set(viable)\n",
    "viable_mask = np.array([any(label in viable_set for label in r[\"categories\"]) for r in rows])\n",
    "viable_idx = np.where(viable_mask)[0]\n",
    "\n",
    "sub_rows = [rows[i] for i in viable_idx]\n",
    "sub_ocr = [text_ocr[i] for i in viable_idx]\n",
    "sub_binarizer = MultiLabelBinarizer(classes=viable)\n",
    "sub_targets = sub_binarizer.fit_transform([r[\"categories\"] for r in sub_rows])\n",
    "sub_density = np.array([numeric_density(t) for t in sub_ocr]).reshape(-1, 1)\n",
    "\n",
    "tr, te = ensure_label_coverage(sub_targets, *split_indices(len(sub_rows)))\n",
    "\n",
    "vectorizers = {\n",
    "    \"char(3,5)\": TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), max_df=0.95),\n",
    "    \"char(2,4)\": TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), max_df=0.95),\n",
    "    \"word\": TfidfVectorizer(analyzer=\"word\", min_df=1, max_df=0.95),\n",
    "    \"word+bi\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), min_df=1, max_df=0.95),\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"logreg\": OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\")),\n",
    "    \"svc\": OneVsRestClassifier(LinearSVC(max_iter=2000)),\n",
    "}\n",
    "\n",
    "for vec_name, vec in vectorizers.items():\n",
    "    feat = vec.fit_transform(sub_ocr)\n",
    "    feat_den = np.hstack([feat.toarray(), sub_density])\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        for feat_label, f in [(vec_name, feat), (f\"{vec_name}+density\", feat_den)]:\n",
    "            clf.fit(f[tr], sub_targets[tr])\n",
    "            raw = (\n",
    "                clf.predict_proba(f[te])\n",
    "                if hasattr(clf, \"predict_proba\")\n",
    "                else (clf.decision_function(f[te]) >= 0).astype(int)\n",
    "            )\n",
    "            preds = (raw >= THRESHOLD).astype(int) if hasattr(clf, \"predict_proba\") else raw\n",
    "            micro = f1_score(sub_targets[te], preds, average=\"micro\", zero_division=0)\n",
    "            macro = f1_score(sub_targets[te], preds, average=\"macro\", zero_division=0)\n",
    "            print(f\"{clf_name:8} {feat_label:30} micro={micro:.3f} macro={macro:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Score \n",
    "\n",
    "Scores and saves the winning vec/clf model after the sweep winner is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_binarizer = MultiLabelBinarizer()\n",
    "text_targets = text_binarizer.fit_transform([r[\"categories\"] for r in rows])\n",
    "\n",
    "text_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), max_df=0.95)\n",
    "text_features = text_vec.fit_transform(text_ocr)\n",
    "\n",
    "tr, te = ensure_label_coverage(text_targets, *split_indices(len(rows)))\n",
    "text_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "text_clf.fit(text_features[tr], text_targets[tr])\n",
    "\n",
    "text_probs = text_clf.predict_proba(text_features)\n",
    "text_df = make_df(text_probs, text_binarizer)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"vectorizer\": text_vec, \"classifier\": text_clf, \"binarizer\": text_binarizer},\n",
    "    ROOT / \"data/models/text.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_df[top_labels(text_df)]\n",
    "text_df[viable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in text_df[top_labels(text_df)]:\n",
    "    plot_hits(top_k(text_df, label, k=8), label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Image model\n",
    "\n",
    "### ResNet18 as a visual feature extractor\n",
    "\n",
    "ResNet was pretrained on natural photographs (ImageNet) and produces embeddings that conflate all screenshots as a generic class--cosine similarity between visually distinct labels was 0.56–0.70, indicating poor separation.\n",
    "\n",
    "Therefore, this model has been removed from the notebook.\n",
    "\n",
    "### CLIP as the image processor\n",
    "\n",
    "CLIP,  `openai/clip-vit-base-patch32`, was pretrained contrastively on web-scraped image-text pairs, which includes UI, screenshots, and documents alongside natural images. The same label pairs that ResNet scored 0.56–0.70 similarity score 0.24–0.45 under CLIP--a meaningful improvement in discriminability across the board.\n",
    "\n",
    "`pooler_output` from the vision encoder is used as the 768-dim embedding.\n",
    "\n",
    "- fixed resolution\n",
    "- no variable patching\n",
    "- CPU-friendly at this corpus size (I only have Intel silicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    use_fast=False,\n",
    "    local_files_only=True,\n",
    ")\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    local_files_only=True,\n",
    ").vision_model\n",
    "clip_model.eval()\n",
    "\n",
    "\n",
    "def embed_images_clip(image_paths, min_size=10):\n",
    "    vectors = []\n",
    "    skipped = []\n",
    "    for path in tqdm(image_paths, desc=\"embedding\"):\n",
    "        with Image.open(path) as img:\n",
    "            if img.size[0] < min_size or img.size[1] < min_size:\n",
    "                skipped.append(path)\n",
    "                vectors.append(np.zeros(768))\n",
    "                continue\n",
    "            inputs = clip_processor(images=img.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            vec = outputs.pooler_output.squeeze(0).cpu().numpy()\n",
    "        vectors.append(vec)\n",
    "    if skipped:\n",
    "        print(f\"skipped {len(skipped)} degenerate images:\")\n",
    "        for p in skipped:\n",
    "            print(f\"  {p.parent.name}/{p.name}\")\n",
    "    return np.vstack(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_binarizer = MultiLabelBinarizer()\n",
    "image_targets = image_binarizer.fit_transform([r[\"categories\"] for r in rows])\n",
    "clip_embeddings = embed_images_clip(paths)\n",
    "\n",
    "print(\n",
    "    f\"samples: {len(rows)}  labels: {len(image_binarizer.classes_)}  \"\n",
    "    f\"dims: {clip_embeddings.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = ensure_label_coverage(image_targets, *split_indices(len(rows)))\n",
    "\n",
    "image_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "image_clf.fit(clip_embeddings[tr], image_targets[tr])\n",
    "\n",
    "image_probs_test = image_clf.predict_proba(clip_embeddings[te])\n",
    "image_preds_test = (image_probs_test >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"micro:\", f1_score(image_targets[te], image_preds_test, average=\"micro\", zero_division=0))\n",
    "print(\"macro:\", f1_score(image_targets[te], image_preds_test, average=\"macro\", zero_division=0))\n",
    "print(\n",
    "    classification_report(\n",
    "        image_targets[te], image_preds_test, target_names=image_binarizer.classes_, zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Score and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_probs = image_clf.predict_proba(clip_embeddings)\n",
    "image_df = make_df(image_probs, image_binarizer)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"classifier\": image_clf, \"binarizer\": image_binarizer}, ROOT / \"data/models/image.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df[top_labels(image_df)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in [\"hockey\", \"browser\", \"food\"]:\n",
    "for label in top_labels(image_df):\n",
    "    plot_hits(top_k(image_df, label, k=8), label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### side-by-side per-label F1 on viable set\n",
    "re-scope both models to the same viable split for a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_binarizer = MultiLabelBinarizer(classes=viable)\n",
    "syn_targets = syn_binarizer.fit_transform([r[\"categories\"] for r in sub_rows])\n",
    "tr, te = ensure_label_coverage(syn_targets, *split_indices(len(sub_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### text winner config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_text_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), max_df=0.95)\n",
    "syn_text_feat = syn_text_vec.fit_transform(sub_ocr)\n",
    "syn_text_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "syn_text_clf.fit(syn_text_feat[tr], syn_targets[tr])\n",
    "syn_text_preds = (syn_text_clf.predict_proba(syn_text_feat[te]) >= THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### image clip embeddings scoped to viable subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clip = clip_embeddings[viable_idx]\n",
    "syn_image_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "syn_image_clf.fit(sub_clip[tr], syn_targets[tr])\n",
    "syn_image_preds = (syn_image_clf.predict_proba(sub_clip[te]) >= THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### per-label F1 for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "text_f1s = f1_score(syn_targets[te], syn_text_preds, average=None, zero_division=0)\n",
    "image_f1s = f1_score(syn_targets[te], syn_image_preds, average=None, zero_division=0)\n",
    "support = syn_targets[te].sum(axis=0)\n",
    "\n",
    "cmp = pd.DataFrame(\n",
    "    {\n",
    "        \"text_f1\": text_f1s,\n",
    "        \"image_f1\": image_f1s,\n",
    "        \"delta\": image_f1s - text_f1s,\n",
    "        \"support\": support.astype(int),\n",
    "    },\n",
    "    index=viable,\n",
    ").sort_values(\"delta\", ascending=False)\n",
    "\n",
    "text_micro = f1_score(syn_targets[te], syn_text_preds, average=\"micro\", zero_division=0)\n",
    "text_macro = f1_score(syn_targets[te], syn_text_preds, average=\"macro\", zero_division=0)\n",
    "image_micro = f1_score(syn_targets[te], syn_image_preds, average=\"micro\", zero_division=0)\n",
    "image_macro = f1_score(syn_targets[te], syn_image_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(cmp.to_string(float_format=\"{:.3f}\".format))\n",
    "print(f\"\\ntext  micro={text_micro:.3f}  macro={text_macro:.3f}\")\n",
    "print(f\"image micro={image_micro:.3f}  macro={image_macro:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Clusters via Unsupervised CLIP Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "this will estimate how long it will take to process all images with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_embed_rate(all_image_paths, embed_fn, n=50):\n",
    "    probe = all_image_paths[:n]\n",
    "    t0 = time.perf_counter()\n",
    "    _ = embed_fn(probe)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    rate = n / elapsed\n",
    "    eta = len(all_image_paths) / rate\n",
    "    print(f\"{n} images in {elapsed:.1f}s - {rate:.1f} it/s\")\n",
    "    print(f\"estimated full run: {eta / 60:.1f} min ({len(all_image_paths)} images)\")\n",
    "\n",
    "\n",
    "# probe_embed_rate(all_image_paths, embed_images_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Compute and Save all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = ROOT / \"data/embeddings/clip_full.pkl\"\n",
    "EMBEDDINGS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    all_clip_embeddings = joblib.load(EMBEDDINGS_PATH)\n",
    "    print(f\"loaded from cache: {all_clip_embeddings.shape}\")\n",
    "else:\n",
    "    all_clip_embeddings = embed_images_clip(all_image_paths)\n",
    "    joblib.dump(all_clip_embeddings, EMBEDDINGS_PATH)\n",
    "    print(f\"computed and saved: {all_clip_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### UMAP Dimensionality Reduction\n",
    "\n",
    "Reduce high-dimensional CLIP embeddings into a 2D representation using UMAP for visualization. The algorithm preserves local neighborhood structure based on cosine similarity, with `n_neighbors=15` controlling locality and `min_dist=0.1` allowing relatively compact clusters. The resulting embedding (`embedding2d`) contains one 2D point per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "embedding2d = reducer.fit_transform(all_clip_embeddings)\n",
    "print(f\"reduced: {embedding2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### Density-Based Clustering with HDBSCAN\n",
    "\n",
    "Apply HDBSCAN to the 2D UMAP embedding to identify clusters based on point density. The algorithm groups regions of sufficient density while labeling sparse regions as noise (`-1`).\n",
    "\n",
    "`min_cluster_size=10` defines the smallest allowable cluster, and `min_samples=5` controls how conservatively dense regions are defined. The output `cluster_labels` assigns each point either a cluster ID or noise. The summary reports the number of detected clusters, the number of noise points, and the total samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric=\"euclidean\")\n",
    "cluster_labels = clusterer.fit_predict(embedding2d)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = (cluster_labels == -1).sum()\n",
    "print(f\"clusters: {n_clusters}  noise points: {n_noise}  total: {len(cluster_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Cluster Visualization in UMAP Space\n",
    "\n",
    "Visualize the 2D UMAP embedding with points colored according to HDBSCAN cluster assignments. Each color corresponds to a density-based cluster detected in the embedding space, while noise points (`-1`) are shown in light grey.\n",
    "\n",
    "The legend reports cluster IDs and their sample counts. The title reflects the total number of images and the number of clusters identified by HDBSCAN (excluding noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(unique_clusters))\n",
    "\n",
    "for i, cid in enumerate(unique_clusters):\n",
    "    mask = cluster_labels == cid\n",
    "    label = f\"noise ({mask.sum()})\" if cid == -1 else f\"cluster {cid} ({mask.sum()})\"\n",
    "    color = \"lightgrey\" if cid == -1 else cmap(i)\n",
    "    ax.scatter(embedding2d[mask, 0], embedding2d[mask, 1], s=6, color=color, label=label, alpha=0.6)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=7, markerscale=2)\n",
    "ax.set_title(f\"UMAP - {len(all_image_paths)} images, {n_clusters} clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "####  inspect a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_cluster(cid, k=12):\n",
    "    indices = [i for i, c in enumerate(cluster_labels) if c == cid]\n",
    "    sample = np.random.RandomState(SEED).choice(indices, size=min(k, len(indices)), replace=False)\n",
    "    hits = [{\"path\": all_image_paths[i], \"score\": clusterer.probabilities_[i]} for i in sample]\n",
    "    plot_hits(hits, columns=4, label=f\"cluster {cid} - n={len(indices)}\")\n",
    "\n",
    "\n",
    "# inspect_cluster(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### gallery sweep, sample from every cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_summary(k=8, columns=4, skip_noise=True):\n",
    "    unique = sorted(c for c in set(cluster_labels) if not (skip_noise and c == -1))\n",
    "    for cid in unique:\n",
    "        indices = [i for i, c in enumerate(cluster_labels) if c == cid]\n",
    "        sample = np.random.RandomState(SEED).choice(\n",
    "            indices, size=min(k, len(indices)), replace=False\n",
    "        )\n",
    "        hits = [{\"path\": all_image_paths[i], \"score\": clusterer.probabilities_[i]} for i in sample]\n",
    "        plot_hits(hits, columns=columns, label=f\"cluster {cid} — n={len(indices)}\")\n",
    "\n",
    "\n",
    "cluster_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Cluster OCR vocabulary vs label OCR vocabulary via Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### map each image path to its cluster id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_cluster = {all_image_paths[i]: int(cluster_labels[i]) for i in range(len(all_image_paths))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### token sets per cluster: union of all OCR words across member images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return set(text.lower().split())\n",
    "\n",
    "\n",
    "cluster_vocab = {}\n",
    "for i, path in enumerate(all_image_paths):\n",
    "    cid = cluster_labels[i]\n",
    "    txt_path = ROOT / \"data/ocr\" / path.parent.name / (path.stem + \".txt\")\n",
    "    if not txt_path.exists():\n",
    "        continue\n",
    "    tokens = tokenize(txt_path.read_text())\n",
    "    cluster_vocab.setdefault(cid, set()).update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### token sets per label: union of OCR words across labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = {}\n",
    "for r, ocr in zip(rows, text_ocr):\n",
    "    tokens = tokenize(ocr)\n",
    "    for label in r[\"categories\"]:\n",
    "        label_vocab.setdefault(label, set()).update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Jaccard similarity: cluster vs label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a, b):\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return len(a & b) / len(a | b)\n",
    "\n",
    "\n",
    "unique_clusters = sorted(c for c in set(cluster_labels) if c != -1)\n",
    "labels_sorted = sorted(label_vocab)\n",
    "\n",
    "jac = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            jaccard(cluster_vocab.get(cid, set()), label_vocab.get(label, set()))\n",
    "            for label in labels_sorted\n",
    "        ]\n",
    "        for cid in unique_clusters\n",
    "    ],\n",
    "    index=[f\"c{cid}\" for cid in unique_clusters],\n",
    "    columns=labels_sorted,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### top 5 label matches per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_matches(jac, cluster_labels, unique_clusters, top_n=5):\n",
    "    print(f\"{'cluster':>8}  {'n':>5}  top label matches\")\n",
    "    print(\"-\" * 80)\n",
    "    for cid in unique_clusters:\n",
    "        n = (cluster_labels == cid).sum()\n",
    "        row = jac.loc[f\"c{cid}\"]\n",
    "        top = row.nlargest(top_n)\n",
    "        desc = \"  |  \".join(f\"{label} ({score:.3f})\" for label, score in top.items())\n",
    "        print(f\"c{cid:>6}  {n:>5}  {desc}\")\n",
    "\n",
    "\n",
    "# print_cluster_matches(jac, cluster_labels, unique_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### column filter: drop labels no cluster scores highly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.10\n",
    "VMAX_PCTILE = 99  # prevents outlier bleaching\n",
    "\n",
    "active_cols = jac.columns[jac.max(axis=0) >= THRESHOLD]\n",
    "J = jac[active_cols].copy()\n",
    "\n",
    "cluster_sizes = {cid: int((cluster_labels == cid).sum()) for cid in unique_clusters}\n",
    "dominant = J.idxmax(axis=1)\n",
    "J.index = [\n",
    "    f\"c{i.lstrip('c'):>2} n={cluster_sizes[int(i.lstrip('c'))]:>3} [{dominant[i]}]\" for i in J.index\n",
    "]\n",
    "\n",
    "vmax = np.percentile(J.values, VMAX_PCTILE)\n",
    "\n",
    "cg = sns.clustermap(\n",
    "    J,\n",
    "    method=\"ward\",\n",
    "    metric=\"euclidean\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    vmin=0,\n",
    "    vmax=vmax,\n",
    "    figsize=(max(14, len(active_cols) * 0.45), max(12, len(J) * 0.22)),\n",
    "    linewidths=0.2,\n",
    "    linecolor=\"#e0e0e0\",\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    dendrogram_ratio=(0.10, 0.06),\n",
    "    cbar_pos=(0.01, 0.84, 0.02, 0.12),\n",
    "    cbar_kws={\"label\": f\"Jaccard (capped p{VMAX_PCTILE})\"},\n",
    ")\n",
    "plt.setp(cg.ax_heatmap.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "plt.setp(cg.ax_heatmap.get_yticklabels(), fontsize=7.5)\n",
    "cg.ax_heatmap.set_xlabel(\"Label\", fontsize=10)\n",
    "cg.ax_heatmap.set_ylabel(\"\")\n",
    "cg.ax_col_dendrogram.set_title(\n",
    "    f\"Cluster × Label — Ward/Euclidean clustermap  (threshold={THRESHOLD})\", fontsize=11\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwatch(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-activity (.venv)",
   "language": "python",
   "name": "image-activity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
